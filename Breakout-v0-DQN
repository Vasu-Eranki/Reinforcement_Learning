import gym
import math 
import torch
import numpy 
import warnings 
import random
import torchvision
import torch.nn.functional as F
from PIL import Image 
from torch import nn,optim
from itertools import count
from collections import namedtuple, deque
#from torchsummary import summary
from torchvision import datasets,transforms
warnings.filterwarnings("ignore")
class HyperParameters:
  LR=5e-3
  GAMMA = 0.999
  RENDER = False
  BATCH_SIZE = 32
  EPOCHS = 50000
  EPS_START = 0.9
  EPS_END = 0.05
  EPS_DECAY = 200
  MEMORY_SIZE = 900000
  TARGET_UPDATE = 10
  class Agent(nn.Module):
  def __init__(self,action_space :int):
    super(Agent,self).__init__()
    #Input is 84x84x4
    self.conv1 = nn.Conv2d(in_channels=4,out_channels=32,kernel_size=8,stride=4,padding=0,padding_mode='zeros')
    self.bn1 = nn.BatchNorm2d(num_features=32)
    #Input is 20x20x32
    self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=4,stride=2,padding=0,padding_mode='zeros')
    self.bn2 = nn.BatchNorm2d(num_features=64)
    #Input is 9x9x64
    self.conv3 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=0,padding_mode='zeros')
    self.bn3 = nn.BatchNorm2d(num_features=64)
    #Input is 7x7x64
    self.flatten = nn.Flatten()
    self.ln1 = nn.Linear(in_features=7*7*64,out_features=512)
    self.ln2 = nn.Linear(in_features=512,out_features=action_space)

  def forward(self,x):
    x=F.relu(self.bn1(self.conv1(x)))
    x=F.relu(self.bn2(self.conv2(x)))
    x=F.relu(self.bn3(self.conv3(x)))
    x=self.flatten(x)
    x=F.relu(self.ln1(x))
    x=F.relu(self.ln2(x))
    return x

Transition = namedtuple('Transition',('state','action','next_state','reward'))
class ReplayMemory(object):
  def __init__(self,capacity):
    self.capacity=capacity
    self.memory = []
    self.position = 0

  def push(self,*args):
    if len(self.memory)<self.capacity: 
      self.memory.append(None)
    self.memory[self.position] = Transition(*args)
    self.position = (self.position+1) % self.capacity
  
  def sample(self,batch_size:int):
    return random.sample(self.memory, batch_size)

  def __len__(self):
    return len(self.memory)
class DQN:
  def __init__(self):
    self.lr=HyperParameters.LR
    self.gamma = HyperParameters.GAMMA
    self.render = HyperParameters.RENDER
    self.batch_size=HyperParameters.BATCH_SIZE
    self.epochs =  HyperParameters.EPOCHS
    self.eps_start = HyperParameters.EPS_START
    self.eps_end = HyperParameters.EPS_END 
    self.eps_decay = HyperParameters.EPS_DECAY
    self.memory_size = HyperParameters.MEMORY_SIZE
    self.target_update = HyperParameters.TARGET_UPDATE
    #Declaring all the hyperparameters requires 
    self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    self.env = gym.make('Breakout-v0')
    self.policy_agent = Agent(self.env.action_space.n)
    self.target_agent = Agent(self.env.action_space.n)
    self.target_agent.load_state_dict(self.policy_agent.state_dict())
    self.target_agent.eval()
    self.optimiser = optim.RMSprop(self.polocy_agent.paramters())
    self.memory = ReplayMemory(self.memory_size)
    self.steps_done = 0
    self.episode_durations =[]
    self.reward_avg = deque([],maxlen=200)

  def select_action(self,state):
    sample = random.random()
    eps_threshold = self.eps_end+(self.eps_star-self.eps_end)*math.exp(-1.*self.steps_done/self.eps_decay)
    self.steps_done+=1
    if sample>eps_thresold:
      with torch.no_grad():
        return self.policy_agent(state).max(1)[1].view(1,1)
    else:
      return torch.tensor([[random.randrange(self.env.action_space.n)],device=self.device,dtype=torch.long])
  
  def optimise_model(self):
    if len(self.memory)<self.batch_size:
      return 
    transitions = self.memory.sample(self.batch_size)
    batch = Transitions(*zip(*transitions))
    non_final_mask = torch.tensor(tuple(map(lambda s:s is not None, batch.next_state)),device=self.device,dtype=torch.uint8)

    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
    state_batch = torch.cat(batch.state)
    action_batch = torch.cat(batch.action)
    reward_batch = torch.cat(batch.reward)

    state_action_values = self.policy_agent(state_batch).gather(1,action_batch)
    next_state_values = torch.zeros(self.batch_size,device=self.device)
    next_state_values[non_final_mask] = self.target_agent(non_final_next_states).max(1)[0].detach()
    expected_state_action_values = (next_state_values*self.gamma)+reward_batch
    loss = F.smooth_l1_loss(state_action_values,expected_state_action_values)
    self.optimiser.zero_grad()
    loss.backward()
    for param in self.policy_agent.paramters():
      param.grad.data.clamp_(-1,1)
    self.optimiser.step()
  @staticmethod
  def PreProcessing(image):
    image = Image.from_array(image)
    image = image.resize([84,84],resample=PIL.Image.Lanzcos)
    image = image.convert(mode="L")
    image = numpy.asarray(image)
    return image

  @staticmethod
  def training():
    for i in range(self.epochs)
    self.env.reset()
    while True:
      action = self.select_action(state)
      _,reward,done,_=self.env.step(action.cpu().item())
      self.reward_avg.append(reward)
      reward = torch.tensor([reward],device=self.device)
      if not done:
        #TO do add preprocessing, adding average reward using deque
      else:

      self.memory.push(state,action,next_state,reward)
      self.optimise_model()
      if done:
        print("Epoch:",self.epoch," Avg Return per Epoch:", numpy.mean(self.reward_avg))
        self.episode_durations.append(t+1)
        break
    if i % self.target_update ==0:
      self.target_agent.load_state_dict(self.policy_agent.state_dict())
def main():
  DeepQ = DQN()
  DeepQ.training()
  print("Complete :) ")
